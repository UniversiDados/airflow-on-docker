# ConfiguraÃ§Ã£o Spark no Airflow - Guia Passo a Passo

## ğŸ“‹ PrÃ©-requisitos
- Airflow instalado e rodando
- Spark instalado no ambiente
- Docker configurado (se aplicÃ¡vel)

## âš™ï¸ Passo 1: Configurar ConexÃ£o Spark no Airflow

### 1.1 Acessar a Interface de AdministraÃ§Ã£o
1. Abra a UI do Airflow no seu navegador
2. No menu superior, clique em **"Admin"**
3. Selecione **"Connections"** no menu dropdown

### 1.2 Criar Nova ConexÃ£o
1. Clique no botÃ£o **"+"** (Add a new record) ou **"Create"**
2. Preencha os campos conforme abaixo:

## ğŸ“ Passo 2: ConfiguraÃ§Ã£o dos Campos da ConexÃ£o

### Campos ObrigatÃ³rios

| Campo | Valor | DescriÃ§Ã£o |
|-------|-------|-----------|
| **Connection Id** | `spark_default` | Identificador Ãºnico da conexÃ£o |
| **Connection Type** | `Spark` | Tipo de conexÃ£o (selecionar da lista) |
| **Host** | `local[*]` | Modo local usando todos os cores disponÃ­veis |
| **Deploy Mode** | `client` | Modo de deploy mais comum para desenvolvimento |
| **Spark Binary** | `spark-submit` | Comando padrÃ£o do Spark |

### Campos Opcionais

| Campo | Valor | ObservaÃ§Ã£o |
|-------|-------|------------|
| **Description** | `ConexÃ£o Spark Local para Docker` | DescriÃ§Ã£o opcional |
| **Port** | *Deixar em branco* | NÃ£o necessÃ¡rio para modo local |
| **YARN Queue** | *Deixar em branco* | NÃ£o utilizando YARN |
| **Kubernetes Namespace** | *Deixar em branco* | NÃ£o utilizando Kubernetes |
| **Principal** | *Deixar em branco* | Apenas se usar Kerberos |
| **Keytab** | *Deixar em branco* | Apenas se usar Kerberos |

## ğŸ”§ Passo 3: ConfiguraÃ§Ãµes Detalhadas

### 3.1 ConfiguraÃ§Ã£o do Host
- **`local[*]`**: Usa todos os cores da mÃ¡quina
- **`local[2]`**: Usa apenas 2 cores (alternativa)
- **`local`**: Usa apenas 1 core

### 3.2 ConfiguraÃ§Ã£o de Deploy Mode
- **`client`**: Recomendado para desenvolvimento local
- **`cluster`**: Para ambientes distribuÃ­dos

## âœ… Passo 4: Salvar e Testar a ConexÃ£o

### 4.1 Salvar ConfiguraÃ§Ã£o
1. ApÃ³s preencher todos os campos necessÃ¡rios
2. Clique em **"Save"** para salvar a conexÃ£o
3. Verifique se a conexÃ£o `spark_default` aparece na lista

### 4.2 Testar ConexÃ£o (Opcional)
1. Na lista de conexÃµes, localize `spark_default`
2. Clique no Ã­cone de **"Test"** (se disponÃ­vel)
3. Verifique se nÃ£o hÃ¡ erros de conectividade

## ğŸš€ Passo 5: Habilitar e Executar a DAG

### 5.1 Localizar a DAG
1. Volte para a pÃ¡gina principal do Airflow
2. Procure pela DAG `pipeline_dados_paises` na lista
3. Aguarde alguns instantes caso ela nÃ£o apareÃ§a imediatamente

### 5.2 Ativar a DAG
1. Localize o toggle (botÃ£o liga/desliga) ao lado do nome da DAG
2. Clique para **ativar** a DAG (deve ficar verde/azul)

### 5.3 Executar Manualmente
1. Clique no nome da DAG `pipeline_dados_paises`
2. No canto superior direito, clique em **"Trigger DAG"**
3. Confirme a execuÃ§Ã£o

## ğŸ“Š Passo 6: Monitorar ExecuÃ§Ã£o

### 6.1 Acompanhar Progresso
1. Na visualizaÃ§Ã£o da DAG, observe o status das tarefas:
   - ğŸŸ¢ **Verde**: Sucesso
   - ğŸ”´ **Vermelho**: Erro
   - ğŸŸ¡ **Amarelo**: Em execuÃ§Ã£o
   - âšª **Branco**: Pendente

### 6.2 Verificar Logs
1. Clique em qualquer tarefa para ver detalhes
2. Selecione **"Log"** para ver mensagens detalhadas
3. Diagnostique erros se necessÃ¡rio

## ğŸ“ Passo 7: Verificar Resultados

### 7.1 Localizar Arquivos de SaÃ­da
ApÃ³s execuÃ§Ã£o bem-sucedida, verifique:
```bash
./data/delta/
â”œâ”€â”€ tabela1/
â”‚   â”œâ”€â”€ _delta_log/
â”‚   â””â”€â”€ *.parquet
â””â”€â”€ tabela2/
    â”œâ”€â”€ _delta_log/
    â””â”€â”€ *.parquet
```

### 7.2 InspeÃ§Ã£o dos Dados
- **Arquivos Parquet**: Dados processados
- **`_delta_log/`**: Logs de transaÃ§Ã£o do Delta Lake
- Verifique se os diretÃ³rios foram criados corretamente

## ğŸ” Troubleshooting Comum

### Problemas de ConexÃ£o
- âœ… Verificar se o Spark estÃ¡ instalado
- âœ… Confirmar que `spark-submit` estÃ¡ no PATH
- âœ… Validar configuraÃ§Ã£o do Docker (se aplicÃ¡vel)

### Problemas de ExecuÃ§Ã£o
- âœ… Verificar logs das tarefas no Airflow
- âœ… Confirmar permissÃµes de escrita no diretÃ³rio `./data/`
- âœ… Validar sintaxe do cÃ³digo Spark

### Problemas de Performance
- âœ… Ajustar `local[*]` para `local[2]` se necessÃ¡rio
- âœ… Monitorar uso de memÃ³ria e CPU
- âœ… Considerar otimizaÃ§Ãµes especÃ­ficas do Spark

## ğŸ“š PrÃ³ximos Passos

1. **Automatizar**: Configure scheduling da DAG
2. **Monitorar**: Configure alertas para falhas
3. **Otimizar**: Ajuste configuraÃ§Ãµes baseado na performance
4. **Escalar**: Considere cluster Spark para dados maiores

---

*ConfiguraÃ§Ã£o concluÃ­da! Sua pipeline Spark estÃ¡ rodando no Airflow.*